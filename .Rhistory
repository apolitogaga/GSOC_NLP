?read.table
getwd()
reuters <- read.table("\reuters.txt.gz", header=TRUE)
reuters <- read.table("reuters.txt.gz", header=TRUE)
View(reuters)
dim(reuters)
head(reuters,2)
summary(reuters)
(N <- dim(reuters)[1])
dim(reuters)
type(reuters$Content)
class(reuters$Content)
(l <- length(levels(reuters$Topic)))
(tops <- table(reuters$Topic))
(selected.tops <- tops[tops>N/l])
(selected.tops <- tops[tops>N/l])
reuters.freq <- reuters[reuters$Topic %in% names(selected.tops),]
reuters.freq
dim(reuters.freq)
table(reuters$Topic)
(N.freq <- dim(reuters.freq)[1])  # new number of rows
levels(reuters.freq$Topic)
reuters.freq[130,]
reuters.freq[134,]
reuters.freq[130,]
reuters.freq[130,]
library(wordcloud)  # for word clouds
library(tm) # for topic modelling
install.packages(wordcloud)
install.packages("wordcloud")
install.packages():
install.packages(tm)
install.packages(tm)
install.packages("SnowballC")
install.packages("e1071")
install.packages("kernlab")
library(wordcloud)  # for word clouds
library(tm) # for topic modelling
require(wordcloud)
library(wordcloud)  # for word clouds
library(tm) # for topic modelling
require(tm)
library(tm) # for topic modelling
require(tm)
install.packages("tn")
install.packages("tm")
library(tm) # for topic modelling
require(tm)
library(tm) # for topic modelling
toy.text <- c("This is a loving first attempt to my loving memories.",
"This a loving second attempt!",
"And a THIRD (3) third attempt!",
"I need you you you ...")
toy.cp <- Corpus(VectorSource(toy.text))
inspect(toy.cp)
?Corpus
reuters.cp <- VCorpus (VectorSource(as.vector(reuters.freq$Content)) )
reuters.cp[5]$content
reuters.cp <- tm_map(reuters.cp, stripWhitespace)
reuters.cp <- tm_map(reuters.cp, content_transformer(tolower))
reuters.cp <- tm_map(reuters.cp, content_transformer(tolower)) # Conversion to lowcase
(my.stopwords <- c(stopwords('english'), 'reuter', 'reuters', 'said'))
my.stopwords <- c(stopwords('english'), 'reuter', 'reuters', 'said')
reuters.cp <- tm_map(reuters.cp, removeWords, my.stopwords)
?tm_map # interface to apply transformation cuntions to corpora.
library(SnowballC) # needed for stemming
wordStem(c("win", "winning", "winner","winners"), language="english")
getStemLanguages()
reuters.cp <- tm_map(reuters.cp, stemDocument, language="english", lazy=TRUE)
tdm <- TermDocumentMatrix (reuters.cp)
tdm <- TermDocumentMatrix (reuters.cp)
inspect(tdm[80:100,20:50]) ## 'inspect' displays detailed information on a corpus or a term-document matrix:
findFreqTerms(tdm, lowfreq=1000)
## now we can form frequency counts
v <- sort(rowSums(as.matrix(tdm)),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v/sum(v))
v
d
v <- sort(rowSums(as.matrix(tdm)),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v/sum(v))
wordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)
wordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)
wordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)
wordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)
word.control <- list(weighting = function(x) weightTfIdf(x, normalize = TRUE))
word.control
tdm2 <- TermDocumentMatrix (reuters.cp, control=word.control)
v <- sort(rowSums(as.matrix(tdm2)),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v/sum(v))
wordcloud(d$word,d$freq,scale=c(8,1),max.words=50, random.order=FALSE)
word.control <- list(weighting = function(x) weightTfIdf(x, normalize = TRUE))
wordcloud(d$word,d$freq,scale=c(8,1),max.words=50, random.order=FALSE)
d$word[1:10]
d$freq[1:10]
selected.tops <- c('ship','sugar')
reuters.ss <- reuters[reuters$Topic %in% selected.tops,]
(N.freq <- dim(reuters.ss)[1])  # new number of rows
reuters.ss$Topic
dim(reuters.ss$Topic)
dim(reuters.ss$Topic)
reuters.ss$Topic
levels(reuters.ss$Topic)
reuters.ss$Topic <- factor(reuters.ss$Topic)  # re-level the factor to have only 2 levels
reuters.ss$Topic
reuters.ss[130,]
reuters.ss[134,]
documents <- reuters.ss$Content
source('reuters-aux.R')
source('reuters-sdaux.R')
source('reuters-aux.R')
strip.text ("I'm going $home to say 3         hellos to my MUM!")
toy.text
(toy.textl <- as.list(toy.text))
toy.bagged <- vector("list", length(toy.textl))
(toy.list <- lapply(toy.textl,strip.text))
toy.bagged
(toy.bow <- lapply(toy.list,table))
documents
(toy.bow <- lapply(toy.list,table))
bagged <- vector("list", length(documents))
bagged <- lapply(documents,strip.text) # produce the stripped text
bagged[[1]]
bagged.bow <- lapply(bagged,table)# produce the bag of words
bagged.bow[[1]]
reuters.BOWs <- make.BoW.frame (bagged.bow)# make it a dataframe zZzZzZzZ
reuters.BOWs[1,"year"]
reuters.BOWs[1,"when"]
reuters.BOWs[1,"year"]
reuters.BOWs[1,"when"]
dim(reuters.BOWs)
reuters.BOWs.tfidf <- tfidf.weight(reuters.BOWs)
View(reuters.BOWs.tfidf)
reuters.BOWs.tfidf <- div.by.euc.length(reuters.BOWs.tfidf)
dim(reuters.BOWs.tfidf)
summary(colSums(reuters.BOWs.tfidf))
reuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf,
select=sapply(colnames(reuters.BOWs.tfidf), FUN=nchar)>2)
dim(reuters.BOWs.tfidf)
reuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf,
select=sapply(colnames(reuters.BOWs.tfidf), FUN=nchar)>2)
dim(reuters.BOWs.tfidf)
(r.3rdQ <- summary(colSums(reuters.BOWs.tfidf))[5])
reuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf,
select=colSums(reuters.BOWs.tfidf)>r.3rdQ)
dim(reuters.BOWs.tfidf)
reuters.definitive
reuters.definitive <- data.frame(Topic=reuters.ss$Topic,reuters.BOWs.tfidf)
dim(reuters.definitive)
summary(reuters.definitive[,1:10])
save(reuters.definitive,file="Reuters-BOWs.RData")
library(e1071) # naiveBayes
set.seed (1234)
N <- nrow(reuters.definitive)
learn <- sample(1:N, round(0.67*N))
nlearn <- length(learn)
ntest <- N - nlearn
reuters.nb <- naiveBayes (Topic ~ ., data=reuters.definitive[learn,], laplace=3)
pred <- predict(reuters.nb,newdata=reuters.definitive[-learn,])
(tt <- table(pred,reuters.definitive$Topic[-learn]))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
baseline <- 100*(1 - max(table(reuters.definitive$Topic))/nrow(reuters.definitive))
100*(baseline-error)/baseline
##
## By HÃ©ctor Apolo Rosales Pulido
## March 2015
## For the Google Summer of Code
## <<R Project for Statistical Computing>>>
## NLP project.
##
##
## Test
##
## Assume that you have a text (try any text) by using tm package load the text
## for processing after that use (Bag of word model) to represent this text
##
##
## Potential tasks
##
# 1- Create a web interface to enter the questions.
# 2- Use (tm package)to read the question.
# 3- Use (NLP package) to convert the question to tokenizers then the matrix finally train
# model and predict score.
# 4- Use UIMA to discover Knowledge.
# 5- Create database that will save answers for the questions.
##
## Skills required
##
# a very good programming experience in R language and Java.
# Good programming Knowledge in HTML.
# Database and SQL.
# Knowledge about tm package and NLP package.
#
# I'm going to work with the reuters dataset.
# unlike the orignal data set this was a little bit modified,
#      all articles with no topics were dropped,
#      and we only have the first topic for each article.
#
library(wordcloud)  # for word clouds
library(tm) # for topic modelling
library(SnowballC) # needed for stemming
library(e1071) # naiveBayes
reuters <- read.table("reuters.txt.gz", header=TRUE)
dim(reuters) # we have 2 columns and 9520 rows
(N <- dim(reuters)[1]) # save the size of our data
reuters$Content <- as.character(reuters$Content) # R loaded it as a factor, we change it to character.
#we get the number of topics.
(l <- length(levels(reuters$Topic)))
#how many topics have each one of them.
(tops <- table(reuters$Topic))
# these are the topcics moer frequent than the average frequency
(selected.tops <- tops[tops>N/l])
## Let's work only with these, for simplicity:
reuters.freq <- reuters[reuters$Topic %in% names(selected.tops),]
## The resulting data frame contains 7,873 news items on 9 topics. The actual news text is the column
## "Content" and its category is the column "Topic". Possible goals are visualizing the data and creating a classifier
## for the news articles (we'll we doing both several times during the course)
(N.freq <- dim(reuters.freq)[1])  # new number of rows
levels(reuters.freq$Topic)
# re-level the factor to have only 9 levels (VERY IMPORTANT)
reuters.freq$Topic <- factor(reuters.freq$Topic)
levels(reuters.freq$Topic)
## an example of a text about 'money-fx'
reuters.freq[130,]
## an example of a text about 'sugar'
reuters.freq[134,]
## some entries are quite long ...
reuters.freq[133,]
## now let's try to do some nice visualization
require(wordcloud)
## We first transform the data to a Corpus to have access to the nice {tm} routines for text manipulation
reuters.cp <- VCorpus (VectorSource(as.vector(reuters.freq$Content)) )
reuters.cp[5]$content
inspect(reuters.cp[1:2])
## so we pre-process the Corpus a bit
reuters.cp <- tm_map(reuters.cp, stripWhitespace) # Elimination of extra whitespaces
reuters.cp <- tm_map(reuters.cp, removeNumbers) # Elimination of numbers
reuters.cp <- tm_map(reuters.cp, removePunctuation) # Elimination of punctuation marks
reuters.cp <- tm_map(reuters.cp, content_transformer(tolower)) # Conversion to lowcase
# Removal of English generic and custom stopwords
my.stopwords <- c(stopwords('english'), 'reuter', 'reuters', 'said')
reuters.cp <- tm_map(reuters.cp, removeWords, my.stopwords)
?tm_map # interface to apply transformation cuntions to corpora.
# Stemming (reducing inflected or derived words to their word stem, base or root form)
reuters.cp <- tm_map(reuters.cp, stemDocument, language="english", lazy=TRUE)
## Convert to TermDocumentMatrix
tdm <- TermDocumentMatrix (reuters.cp)
inspect(tdm[80:100,20:50]) ## 'inspect' displays detailed information on a corpus or a term-document matrix:
# So we really have a very sparse data representation
# inspect most popular words
findFreqTerms(tdm, lowfreq=1000)
## now we can form frequency counts
v <- sort(rowSums(as.matrix(tdm)),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v/sum(v))
# do
wordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)
## Now let's do something better; we generate a new term-document matrix by
## TfIdf (weight by term frequency - inverse document frequency)
word.control <- list(weighting = function(x) weightTfIdf(x, normalize = TRUE))
tdm2 <- TermDocumentMatrix (reuters.cp, control=word.control)
v <- sort(rowSums(as.matrix(tdm2)),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v/sum(v))
## which possibly gives a better impression of the data, but this is subjective
wordcloud(d$word,d$freq,scale=c(8,1),max.words=50, random.order=FALSE)
## Let's work only with a couple of topics, for simplicity:
selected.tops <- c('ship','sugar')
reuters.ss <- reuters[reuters$Topic %in% selected.tops,]
## The goal is to classify (and be able to predict) news documents as dealing with ships or with sugar
## The resulting data now contains 305 news items
(N.freq <- dim(reuters.ss)[1])  # new number of rows
reuters.ss$Topic <- factor(reuters.ss$Topic)  # re-level the factor to have only 2 levels
## an example of a text about 'ship'
reuters.ss[130,]
## an example of a text about 'sugar'
reuters.ss[134,]
## One of the most used ML representations for text is the "bag of words"
documents <- reuters.ss$Content
## first we need some auxiliary functions to actually create the structure
source('reuters-aux.R')
## "pre-allocate" an empty list of the required length
bagged <- vector("list", length(documents))
## Now generate bag of words as a list
bagged <- lapply(documents,strip.text) # produce the stripped text
bagged[[1]]
bagged.bow <- lapply(bagged,table)# produce the bag of words
bagged.bow[[1]]
reuters.BOWs <- make.BoW.frame (bagged.bow)# make it a dataframe zZ
reuters.BOWs[1,"year"]
reuters.BOWs[1,"when"]
dim(reuters.BOWs)
## we have 305 news entries "described" by nearly 2,800 features (the words)
# Now we weight by inverse document frequency
reuters.BOWs.tfidf <- tfidf.weight(reuters.BOWs)
View(reuters.BOWs.tfidf)
# and normalize by vector length
reuters.BOWs.tfidf <- div.by.euc.length(reuters.BOWs.tfidf)
dim(reuters.BOWs.tfidf)
# let's inspect the result
summary(colSums(reuters.BOWs.tfidf))
## too many columns
# remove those words shorter than 3 characters
reuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf,
select=sapply(colnames(reuters.BOWs.tfidf), FUN=nchar)>2)
dim(reuters.BOWs.tfidf)
# remove those words whose total sum is not greater than the third quartile of the distribution
(r.3rdQ <- summary(colSums(reuters.BOWs.tfidf))[5])
reuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf,
select=colSums(reuters.BOWs.tfidf)>r.3rdQ)
# obviously this is roughly a further 75% reduction, corresponding to the less represented words
dim(reuters.BOWs.tfidf)
# Add class labels back (the "Topics")
# (the normalizing and weighting functions don't work well with non-
# numeric columns so it's simpler to add the labels at the end)
reuters.definitive <- data.frame(Topic=reuters.ss$Topic,reuters.BOWs.tfidf)
dim(reuters.definitive)
# a final look at the first 10 variables
summary(reuters.definitive[,1:10])
## Naive Bayes Classifier
set.seed (1234)
N <- nrow(reuters.definitive)
learn <- sample(1:N, round(0.67*N))
nlearn <- length(learn)
ntest <- N - nlearn
reuters.nb <- naiveBayes (Topic ~ ., data=reuters.definitive[learn,], laplace=3)
# predict the left-out data
pred <- predict(reuters.nb,newdata=reuters.definitive[-learn,])
(tt <- table(pred,reuters.definitive$Topic[-learn]))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# so error is 17.8%, although there is a marked tendency to predict 'sugar'. The majority class is:
baseline <- 100*(1 - max(table(reuters.definitive$Topic))/nrow(reuters.definitive))
# so all errors below 42% are an improvement over the baseline
100*(baseline-error)/baseline
# actually we are able to get a relative reduction of 57.5% in error
## However note that this result is highly unstable,
##   given the small size of both the learn and test sets
(error <- 100*(1-sum(diag(tt))/sum(tt)))
baseline <- 100*(1 - max(table(reuters.definitive$Topic))/nrow(reuters.definitive))
100*(baseline-error)/baseline
baseline
install.packages("NPL")
install.packages("NLP")
install.packages("NLP")
version
