{
    "contents" : "## \n## By HÃ©ctor Apolo Rosales Pulido\n## March 2015\n## For the Google Summer of Code\n## <<R Project for Statistical Computing>>>\n## NLP project.\n##\n\n##\n## Test\n##\n## Assume that you have a text (try any text) by using tm package load the text \n## for processing after that use (Bag of word model) to represent this text\n## \n\n##\n## Potential tasks\n## \n# 1- Create a web interface to enter the questions.\n# 2- Use (tm package)to read the question.\n# 3- Use (NLP package) to convert the question to tokenizers then the matrix finally train\n# model and predict score.\n# 4- Use UIMA to discover Knowledge.\n# 5- Create database that will save answers for the questions.\n\n##\n## Skills required \n##\n# a very good programming experience in R language and Java.\n# Good programming Knowledge in HTML.\n# Database and SQL.\n# Knowledge about tm package and NLP package.\n\n\n#\n# I'm going to work with the reuters dataset.\n# unlike the orignal data set this was a little bit modified, \n#      all articles with no topics were dropped, \n#      and we only have the first topic for each article.\n# \n\nlibrary(wordcloud)  # for word clouds\nlibrary(tm) # for topic modelling\nlibrary(SnowballC) # needed for stemming\nlibrary(e1071) # naiveBayes\n\nreuters <- read.table(\"reuters.txt.gz\", header=TRUE)\ndim(reuters) # we have 2 columns and 9520 rows\n(N <- dim(reuters)[1]) # save the size of our data\n\nreuters$Content <- as.character(reuters$Content) # R loaded it as a factor, we change it to character.\n#we get the number of topics.\n(l <- length(levels(reuters$Topic)))\n#how many topics have each one of them.\n(tops <- table(reuters$Topic))\n\n# these are the topcics moer frequent than the average frequency\n(selected.tops <- tops[tops>N/l])\n\n## Let's work only with these, for simplicity:\nreuters.freq <- reuters[reuters$Topic %in% names(selected.tops),]\n\n## The resulting data frame contains 7,873 news items on 9 topics. The actual news text is the column \n## \"Content\" and its category is the column \"Topic\". Possible goals are visualizing the data and creating a classifier\n## for the news articles (we'll we doing both several times during the course)\n\n(N.freq <- dim(reuters.freq)[1])  # new number of rows\n\nlevels(reuters.freq$Topic)\n# re-level the factor to have only 9 levels (VERY IMPORTANT)\nreuters.freq$Topic <- factor(reuters.freq$Topic)\nlevels(reuters.freq$Topic)\n\n## an example of a text about 'money-fx'\nreuters.freq[130,]\n## an example of a text about 'sugar'\nreuters.freq[134,]\n\n## some entries are quite long ...\nreuters.freq[133,]\n## We first transform the data to a Corpus to have access to the nice {tm} routines for text manipulation\nreuters.cp <- VCorpus (VectorSource(as.vector(reuters.freq$Content)) )\nreuters.cp[5]$content\ninspect(reuters.cp[1:2])\n\n## so we pre-process the Corpus a bit\nreuters.cp <- tm_map(reuters.cp, stripWhitespace) # Elimination of extra whitespaces\nreuters.cp <- tm_map(reuters.cp, removeNumbers) # Elimination of numbers\nreuters.cp <- tm_map(reuters.cp, removePunctuation) # Elimination of punctuation marks\nreuters.cp <- tm_map(reuters.cp, content_transformer(tolower)) # Conversion to lowcase \n\n# Removal of English generic and custom stopwords\nmy.stopwords <- c(stopwords('english'), 'reuter', 'reuters', 'said')\nreuters.cp <- tm_map(reuters.cp, removeWords, my.stopwords)\n?tm_map # interface to apply transformation cuntions to corpora.\n\n# Stemming (reducing inflected or derived words to their word stem, base or root form)\nreuters.cp <- tm_map(reuters.cp, stemDocument, language=\"english\", lazy=TRUE)\n\n## Convert to TermDocumentMatrix\ntdm <- TermDocumentMatrix (reuters.cp)\ninspect(tdm[80:100,20:50]) ## 'inspect' displays detailed information on a corpus or a term-document matrix:\n# So we really have a very sparse data representation\n# inspect most popular words\nfindFreqTerms(tdm, lowfreq=1000)\n\n## now we can form frequency counts\nv <- sort(rowSums(as.matrix(tdm)),decreasing=TRUE)\nd <- data.frame(word = names(v),freq=v/sum(v))\n# do\nwordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)\n\n## Now let's do something better; we generate a new term-document matrix by \n## TfIdf (weight by term frequency - inverse document frequency)\nword.control <- list(weighting = function(x) weightTfIdf(x, normalize = TRUE))\ntdm2 <- TermDocumentMatrix (reuters.cp, control=word.control)\n\nv <- sort(rowSums(as.matrix(tdm2)),decreasing=TRUE)\nd <- data.frame(word = names(v),freq=v/sum(v))\n\n## which possibly gives a better impression of the data, but this is subjective\nwordcloud(d$word,d$freq,scale=c(8,1),max.words=50, random.order=FALSE)\n\n\n## Let's work only with a couple of topics, for simplicity:\nselected.tops <- c('ship','sugar')\nreuters.ss <- reuters[reuters$Topic %in% selected.tops,]\n## The goal is to classify (and be able to predict) news documents as dealing with ships or with sugar\n## The resulting data now contains 305 news items\n(N.freq <- dim(reuters.ss)[1])  # new number of rows\nreuters.ss$Topic <- factor(reuters.ss$Topic)  # re-level the factor to have only 2 levels\n## an example of a text about 'ship'\nreuters.ss[130,]\n## an example of a text about 'sugar'\nreuters.ss[134,]\n\n## One of the most used ML representations for text is the \"bag of words\"\ndocuments <- reuters.ss$Content\n\n## first we need some auxiliary functions to actually create the structure\nsource('reuters-aux.R')\n\n## \"pre-allocate\" an empty list of the required length\nbagged <- vector(\"list\", length(documents))\n\n## Now generate bag of words as a list\nbagged <- lapply(documents,strip.text) # produce the stripped text\nbagged[[1]]\nbagged.bow <- lapply(bagged,table)# produce the bag of words\nbagged.bow[[1]]\nreuters.BOWs <- make.BoW.frame (bagged.bow)# make it a dataframe zZ\nreuters.BOWs[1,\"year\"]\nreuters.BOWs[1,\"when\"]\ndim(reuters.BOWs)\n## we have 305 news entries \"described\" by nearly 2,800 features (the words)\n# Now we weight by inverse document frequency\nreuters.BOWs.tfidf <- tfidf.weight(reuters.BOWs)\n\n# and normalize by vector length\nreuters.BOWs.tfidf <- div.by.euc.length(reuters.BOWs.tfidf)\ndim(reuters.BOWs.tfidf)\n\n# let's inspect the result\nsummary(colSums(reuters.BOWs.tfidf))\n\n## too many columns\n# remove those words shorter than 3 characters\nreuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf, \n                        select=sapply(colnames(reuters.BOWs.tfidf), FUN=nchar)>2)\ndim(reuters.BOWs.tfidf)\n\n# remove those words whose total sum is not greater than the third quartile of the distribution\n(r.3rdQ <- summary(colSums(reuters.BOWs.tfidf))[5])\nreuters.BOWs.tfidf <- subset.data.frame (reuters.BOWs.tfidf,\n                                         select=colSums(reuters.BOWs.tfidf)>r.3rdQ)\n# obviously this is roughly a further 75% reduction, corresponding to the less represented words\ndim(reuters.BOWs.tfidf)\n\n# Add class labels back (the \"Topics\")\n# (the normalizing and weighting functions don't work well with non-\n# numeric columns so it's simpler to add the labels at the end)\nreuters.definitive <- data.frame(Topic=reuters.ss$Topic,reuters.BOWs.tfidf)\ndim(reuters.definitive)\n# a final look at the first 10 variables\nsummary(reuters.definitive[,1:10])\n\n## Naive Bayes Classifier\nset.seed (1234)\nN <- nrow(reuters.definitive)                                                                                              \nlearn <- sample(1:N, round(0.67*N))\nnlearn <- length(learn)\nntest <- N - nlearn\nreuters.nb <- naiveBayes (Topic ~ ., data=reuters.definitive[learn,], laplace=3)\n# predict the left-out data\npred <- predict(reuters.nb,newdata=reuters.definitive[-learn,])\n(tt <- table(pred,reuters.definitive$Topic[-learn]))\n(error <- 100*(1-sum(diag(tt))/sum(tt)))\n# so error is 17.8%, although there is a marked tendency to predict 'sugar'. The majority class is:\n(baseline <- 100*(1 - max(table(reuters.definitive$Topic))/nrow(reuters.definitive)))\n# so all errors below 42% are an improvement over the baseline\n100*(baseline-error)/baseline\n# actually we are able to get a relative reduction of 57.5% in error\n## However note that this result is highly unstable, \n##   given the small size of both the learn and test sets",
    "created" : 1427299479173.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3094321499",
    "id" : "54AC0219",
    "lastKnownWriteTime" : 1427328410,
    "path" : "~/Documents/dev/github/gsoc_NLP/GSOC_NLP/TestHectorRosales.R",
    "project_path" : "TestHectorRosales.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}